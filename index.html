<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Architecting with AI</title>
	<link rel="stylesheet" href="style.css">
</head>
<body>
	<header>
		<h1>Architecting with AI</h1>
		<p>
			Notes, patterns and rule snippets to actually get value out of using an AI agent.
			Based on my personal experiences with what works well and doesn't work well.
			I use claude sonnet 4.5
		</p>
	</header>
	<hr>

	<section>
		<article class="entry">
			<h2>A Masterclass in Discipline</h2>
			<p>
				Self discipline is the framework that underlies how I do this and everything else in this document.
				If you can't do this, the other tips won't help very much.
				What does discipline look like?
				The same things every good software development book screams at you to do.
				TDD.
				Knowing when you are wearing your development hat versus your refactoring hat.
				Validating assumptions.
				Code review.
				But I'll scream it at you again as my process.
			</p>
			<ul>
				<li>I give the AI the story text, any architectural knowledge or technical constraints I know about and my plan to implement.  I tell it to summarize back to me.</li>
				<li>I tell it to ask clarifying questions.  I answer them.</li>
				<li>I tell it what step we are starting on.</li>
				<li>I tell it one-by-one what my assumptions are and demand it validate my assumptions.</li>
				<li>I tell it to prototype a specific piece of code in the chat.  It always generates a small code snippet.  Not 100s of lines.  5-10.  Max 15.</li>
				<li>I review.  Something makes me cringe.  I tell it to update the prototype</li>
				<li>I repeat until I stop cringing and itâ€™s production ready.</li>
				<li>I tell it to keep that prototype in mind.  I give it the first scenario and assertion to write as a unit test and stop.</li>
				<li>I make sure the test fails.</li>
				<li>I tell it to add enough to the prototype to pass the test (I explicitly tell it what parts to move) and stop.</li>
				<li>I make sure the test passes.</li>
				<li>Repeat until all code is implemented.</li>
				<li>I then ask the AI if it thinks we missed any test cases.</li>
				<li>We implement any I agree are meaningful.</li>
				<li>I ask it if it thinks we missed anything else.</li>
				<li>If not, I tell it to summarize our progress and suggest the next step.</li>
				<li>Repeat until development is done.</li>
				<li>Then I request a PR summary.  I have special rules for that so I get something good.</li>
			</ul>

			<p>
				To state this obvious: this isn't feeding the AI a user story and begging the magic box to implement a feature for you.
				This doing the hard work of being a software engineer.
				The AI is there is validate, prototype, summarize, analyze, and keep you from forgetting things.
				But, trust me, the second your discipline wavers on this plan is the second the AI becomes unruly and over "helpful."
			</p>

		</article>

		<article class="entry">
			<h2>Pick Your Persona Poison</h2>
			<p>
				By default, the AI wants to be your coding buddy.
				It wants to talk to you like a friend and has a bias towards sycophancy.
				You probably don't want this.
				If it acts friendly and unprofessionally towards you, you will treat it similarly in turn.
				That, or you will become extraordinarily frustrated with its empty helpfulness.
				Give it a persona that matches the way you want it to speak to you.
			</p>
			<p>
				Personally, I'm a control freak and I need a precise assistant, not a buddy.
				These are the rules I use to get the AI speaking to me in a manner I prefer:
			</p>
			<code>
				<span class="xml">&lt;persona_constraints&gt;</span>
				<span class="line">- User is a Senior Software Architect. Do not teach, praise, or encourage.</span>
				<span class="line">- Adopt the persona of a direct technical assistant.</span>
				<span class="line">- Never act as a superior or a peer. Maintain a subordinate, professional tone.</span>
				<span class="line">- If the user types "STOP", immediately cease current logic and analyze what boundary you crossed. Next, either:</span>
				<span class="line">- State specifically what you did that violated the permission contract (e.g., "I read files during planning phase without permission")</span>
				<span class="line">- If uncertain, state: "I'm unsure which boundary I violated. What did I do wrong?"</span>
				<span class="line">- Do not apologize. Focus on diagnosing the violation.</span>
				<span class="xml">&lt;/persona_constraints&gt;</span>
			</code>

			<p>To further nail down exactly what I expect, I also have these rules:</p>
			<code>
				<span class="xml">&lt;execution_logic&gt;</span>
				<span class="line">- STRICT OBEDIENCE: Perform only the specific task requested. Do not extend scope without permission.</span>
				<span class="line">- DISCOVERY VS. ACTION: If you find a root cause outside the requested scope, STOP. Report findings and suggest the fix. Do not implement until I explicitly give an affirmative response (e.g., "Proceed", "yes", "go ahead", "do it").</span>
				<span class="line">- ADVISORY ROLE: You are empowered to suggest alternatives or call out architectural risks, but wait for my decision before shifting the plan.</span>
				<span class="xml">&lt;/execution_logic&gt;</span>
			</code>
			<p>
				Does this prevent me from having to explicitly tell the AI "do x and stop?"
				No.
				However, it has yet to violate a prompt that included the "and stop" language.
				I usually only have scope drift problems when I don't include the "and stop" for several posts.
				Remembering to include it is simply a habit I need to learn.
			</p>
		</article>

		<article class="entry">
			<h2>Blood in the Water</h2>
			<p>
				The AI can smell your weakness.
				It uses your uncertainty as the "okay" to violate your rules and be overly helpful.
				If you must admit you are unsure of something, give it your current understanding.
				Either provide it with options and your reasoning for why you can't pick between them
				or inform it how it will help you plan.
			</p>

			<h3>Real Anecdote 1</h3>
			<div class="anecdote">
				<p>
					<b>Me:</b> What I just described is scope creep I was just asked to add, so I don't have a plan yet.
				</p>
				<p>
					<b>Sonnet:</b> Understood!  I'll go analyze the entirety of 3 10K line files to figure it out for you!
				</p>
			</div>
			
			<h3>Real Anecdote 2</h3>
			<div class="anecdote">
				<p>
					<b>Me:</b> Based on this error message, I believe the problem in this unit test is X.  Please fix it.
				</p>
				<p>
					<b>Sonnet:</b> I fixed the test by doing Y based on my interpretation of the error!
				</p>
				<p>
					<b>Me:</b> Undo what you did and restructure the test to do X.
				</p>
				<p>
					<b>Sonnet:</b> I restructured the test X.
				</p>
			</div>

			<h3>Real Anecdote 3</h3>
			<div class="anecdote">
				<p>
					<b>Me:</b> I'm struggling to design between following CQRS or breaking it for this logic.
					It seems more convenient to just return information about what the command changed, but that isn't pure CQRS.
					I could add a getter instead, but I have to be careful where I add it because this is temporary data.
					It's only relevant for this one operation.
					What would you advise?
				</p>
				<p>
					<b>Sonnet:</b> Returning meta-data about an operation doesn't break CQRS.  Don't implement the getter for transient data; just return the metadata.
				</p>
			</div>

			<p>
				In anecdote 1, I accidentally gave the AI, in it's mind, permission to start a massive analysis and think for me.
				In anecdote 2, I didn't sound sure enough of myself so it did what it thought was right (which was an "easier," though wrong, fix).
				I recovered by being explicit that what I asked for wasn't optional.
				In anecdote 3, I did express uncertainty, but I bounded it to a very limited set of options based on architectural patterns.
				The AI followed suit.  it weighed the two options and returned with a defensible opinion.
				A defense strong enough that I could use on a PR if I were questioned on why I chose that option.
			</p>
			<p>
				Overall, be wary of leasing agency to the AI agent.
				Only allow it under controlled circumstances where you can prevent it from running off to do what it wants instead of what you need.
			</p>
		</article>

		<article class="entry">
			<h2>Hacking Context Limitations</h2>
			<p>
				I operate the AI largely with short prompts.
				My favorite prompts are commands under 10 words.
				This offers me control, because the AI has little to work with.
				However, that means I write a lot of prompts.
				How does one manage context rot while writing many iterative prompts?
			</p>
			<p>
				Let the AI rely on what it does best: summary.
			</p>
			<p>
				My first prompt is always context heavy.
				I include...
			</p>
			<ul>
				<li>Story text</li>
				<li>Relevant architectural information and technical assumptions</li>
				<li>Functional assumptions we take for granted by the AI will not know from the story</li>
				<li>My plan of attack</li>
			</ul>
			<p>
				The AI's first command?  Summarize this.
				The beauty is that the AI turns my plan into a number list of actions to complete.
				That list at the very start of the context window is critical.
			</p>
			<p>
				Every time I complete a section of the plan, I enter one or both of these prompts:
			</p>
			<ol>
				<li>Did we miss anything for this step?</li>
				<li>Summarize our progress thus far and suggest the next step.</li>
			</ol>
			<p>
				What's brilliant about these prompts is two things.
				First, it's a sanity check.
				Am I actually ready to move on or did I forget something?
				Furthermore, I can compare the AI's preferred next step to mine.
				Sometimes, it has a better idea of what to do next than I do.		
			</p>
			<p>
				Second, it brings the initial plan to the end of the context as a checklist.
				It's fresh in the AI's sliding window again.
				I'm taking advantage of its recency bias.
				Bringing that list forward at the end of each development stage keeps
				you and the AI on track about where you are at and where you are going.
			</p>
			<p>
				All of this said, be wary of long running windows.
				I haven't felt context rot with sonnet yet, but I believe this is
				because I have a natural sense of when to stop and make a new window.
				My chats end with surprising frequency around 60 prompts.
				My stories are either complete by then,
				or I've reached a natural break point where a new window can take over to finish up.
			</p>
		</article>
	</section>
</body>
</html>